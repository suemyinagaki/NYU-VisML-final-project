{"cells":[{"cell_type":"markdown","source":["### download the required packages"],"metadata":{"id":"oFP00g3OYnGO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdpeNyL6SEbS"},"outputs":[],"source":["!pip3 install -r requirements.txt\n","!pip install onnxruntime==1.12.0\n","!pip3 install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","!pip3 install cython_bbox\n","!pip3 install faiss-cpu\n","!pip3 install faiss-gpu\n","!python3 setup.py develop\n","!pip install loguru\n","!pip install thop\n","!pip install lap\n","!pip install yacs"]},{"cell_type":"markdown","source":["### run the bot-sort  re-identification model\n","- For the appearance extractor, use the SBS-S50 fine-tuned on MOT17\n","- use the default parameters of MOT17"],"metadata":{"id":"zBiTQWoYYuI2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HW3zWM1wIsS"},"outputs":[],"source":["!python3 tools/track.py \"/content/drive/My Drive/visML_project/streetaware_sample_chase_1_sensor_1_right\" --default-parameters --fast-reid-weights pretrained/mot17_sbs_S50.pth --benchmark \"MOT17\" --eval \"test\" --fp16 --fuse"]},{"cell_type":"markdown","source":["### Draw the detection outputs on the corresponding frame\n","- the Bot-Sort output is a txt file where each line has the format: \\<frame_id>, \\<identity number>, \\<bounding box left> , \\<bounding box top>, \\<bounding box width>, \\<bounding box height>, and \\<confidence score>"],"metadata":{"id":"WXl1DrdW4o0G"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os\n","\n","# Load tracking data\n","track_file = '/content/drive/My Drive/visML_project/outputs/botsort/chase_1/sensor_1/right_people_track.txt'\n","tracks = {}\n","with open(track_file, 'r') as file:\n","    for line in file:\n","        frame_id, track_id, x, y, w, h, conf, _, _, _ = map(float, line.strip().split(','))\n","        if frame_id not in tracks:\n","            tracks[frame_id] = []\n","        tracks[frame_id].append((track_id, x, y, w, h, conf))\n","\n","# Assign a unique color to each track_id\n","color_map = {}\n","for frame in tracks.values():\n","    for track_id, *_ in frame:\n","        if track_id not in color_map:\n","            color_map[track_id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n","\n","# Directory containing frames\n","frames_dir = '/content/drive/My Drive/visML_project/streetaware_sample_chase_1_sensor_1_right/test/MOT17-01-FRCNN/img1/'\n","\n","# Output directory\n","output_dir = '/content/drive/My Drive/visML_project/output_frames/sensor_2_left'\n","\n","# Process each frame\n","for frame_file in sorted(os.listdir(frames_dir)):\n","    frame_path = os.path.join(frames_dir, frame_file)\n","    frame = cv2.imread(frame_path)\n","    if frame is None:\n","        continue\n","\n","    frame_id = int(frame_file.split('.')[0])\n","    if frame_id in tracks:\n","        for track_id, x, y, w, h, conf in tracks[frame_id]:\n","            color = color_map[track_id]\n","            cv2.rectangle(frame, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n","            cv2.putText(frame, f'ID: {int(track_id)}, Conf: {conf:.2f}', (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","\n","    # Save the frame\n","    output_path = os.path.join(output_dir, frame_file)\n","    cv2.imwrite(output_path, frame)\n"],"metadata":{"id":"6ORuAwhp5nvw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Code to visualize the bounding box and confidence score of the original detection file"],"metadata":{"id":"Hrj_FeUT86oa"}},{"cell_type":"code","source":["# load the file:\n","json_file_path = 'StreetAware/sensor_1/left_people.json'\n","with open(json_file_path, 'r') as file:\n","    data = json.load(file)"],"metadata":{"id":"2d_iUVeI9Jtd","executionInfo":{"status":"ok","timestamp":1715301822617,"user_tz":240,"elapsed":3,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1iBpfiJycMq39fNWAWdR2KxDH-N2L7iLs"},"executionInfo":{"elapsed":13353,"status":"ok","timestamp":1714015511679,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"sNxHbDWGGd4L","outputId":"c1b627b8-ec5f-45cf-c252-b2cd20d89976"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["json_file_path = 'StreetAware/sensor_1/left_people.json'\n","with open(json_file_path, 'r') as file:\n","    data = json.load(file)import cv2\n","import os\n","import json\n","from google.colab.patches import cv2_imshow\n","\n","# Load the JSON data\n","with open('/content/drive/My Drive/visML_project/StreetAware/sensor_1/left_people.json', 'r') as file:\n","    data = json.load(file)\n","\n","# Directory containing frames\n","frames_dir = '/content/drive/My Drive/visML_project/StreetAware/sensor_1/img_left/'\n","\n","# Output directory\n","output_dir = '/content/drive/My Drive/visML_project/output_frames'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Initialize counter for non-null frames\n","non_null_counter = 0\n","max_non_null_frames = 10\n","\n","# Scaling factor\n","scale_factor = 0.5\n","\n","# Process each item in the JSON data\n","for frame_index, frame_data in enumerate(data):\n","    if frame_data is None:\n","        continue\n","\n","    # Increment the counter and break if maximum frames are processed\n","    non_null_counter += 1\n","    if non_null_counter > max_non_null_frames:\n","        break\n","\n","    frame_file = f'{frame_index + 1:06d}.jpg'\n","    frame_path = os.path.join(frames_dir, frame_file)\n","    frame = cv2.imread(frame_path)\n","\n","    if frame is None:\n","        print(f\"Frame {frame_file} not found.\")\n","        continue\n","\n","    # Resize the frame\n","    frame = cv2.resize(frame, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n","\n","    # Draw bounding boxes and display confidence\n","    scores = frame_data.get('scores', [])\n","    for box_index, box in enumerate(frame_data['boxes']):\n","        top_left = (int(box[0][0] * scale_factor), int(box[0][1] * scale_factor))\n","        bottom_right = (int(box[1][0] * scale_factor), int(box[1][1] * scale_factor))\n","\n","        # Determine the color based on the availability of a score\n","        if box_index < len(scores):\n","            color = (0, 255, 0)\n","            score = scores[box_index]\n","            text = f'{score:.2f}'\n","        else:\n","            color = (0, 0, 255)\n","            text = 'N/A'\n","\n","        cv2.rectangle(frame, top_left, bottom_right, color, 2)\n","        cv2.putText(frame, text, (top_left[0], top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","\n","    # Display the frame\n","    output_path = os.path.join(output_dir, frame_file)\n","    cv2.imwrite(output_path, frame)\n","    cv2_imshow(frame)\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","source":["### Code to convert the frames to a video"],"metadata":{"id":"xfGR716f9rGq"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140556,"status":"ok","timestamp":1714086943441,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"Y69bzD26-X55","outputId":"8a2193c8-677c-46e4-d4b4-ba47cee13875"},"outputs":[{"name":"stdout","output_type":"stream","text":["Video saved to output_videos/sensor_2_left.mp4\n"]}],"source":["import cv2\n","import os\n","\n","# Directory containing frames\n","frames_dir = 'output_frames/sensor_2_left/'\n","\n","# Output directory\n","output_dir = 'output_videos/'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","output_video_path = os.path.join(output_dir, 'sensor_2_left.mp4')\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","fps = 30\n","\n","first_frame_path = os.path.join(frames_dir, '000001.jpg')\n","first_frame = cv2.imread(first_frame_path)\n","frame_height, frame_width = first_frame.shape[:2]\n","video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n","\n","# Process each frame\n","sorted_frame_files = sorted(os.listdir(frames_dir))\n","for frame_file in sorted_frame_files:\n","    if frame_file.endswith('.jpg'):\n","        frame_path = os.path.join(frames_dir, frame_file)\n","        frame = cv2.imread(frame_path)\n","        if frame is None:\n","            continue\n","        video_writer.write(frame)\n","\n","video_writer.release()\n","print(f\"Video saved to {output_video_path}\")\n"]},{"cell_type":"markdown","source":["### Calculate the evaluation metrics and save to a json file"],"metadata":{"id":"-_Dbvwiz-BdC"}},{"cell_type":"code","source":["def box_overlap_count(boxes):\n","    def overlap(box1, box2):\n","        # Check if box1 overlaps with box2\n","        x1_min, y1_min, x1_max, y1_max = box1\n","        x2_min, y2_min, x2_max, y2_max = box2\n","        return not (x1_max < x2_min or x2_max < x1_min or y1_max < y2_min or y2_max < y1_min)\n","\n","    count = 0\n","    for i in range(len(boxes)):\n","        for j in range(i + 1, len(boxes)):\n","            if overlap(boxes[i], boxes[j]):\n","                count += 1\n","    return count\n"],"metadata":{"id":"oYq_zwDGo-05"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFKHVIBWiT4T"},"outputs":[],"source":["import json\n","\n","video_name = \"streetaware_sample_chase_2_sensor_2_right\"\n","txt_path = 'outputs/botsort/chase_2/sensor_2/right_people_track.txt'\n","output_json = 'outputs/botsort/chase_2/sensor_2/right_people_track.json'\n","frame_data = {}\n","\n","# Reading data from file\n","with open(txt_path, 'r') as file:\n","    data = file.read()\n","\n","for line in data.strip().split(\"\\n\"):\n","    parts = line.split(',')\n","    frame_id = int(parts[0])\n","    people_id = int(parts[1])\n","    x1 = float(parts[2])\n","    y1 = float(parts[3])\n","    width = float(parts[4])\n","    height = float(parts[5])\n","    conf = float(parts[6])\n","    x2 = x1 + width\n","    y2 = y1 + height\n","    bbox_area = width * height\n","    if frame_id not in frame_data:\n","        frame_data[frame_id] = []\n","    frame_data[frame_id].append([x1, y1, x2, y2, people_id, bbox_area, conf])"]},{"cell_type":"code","source":["\n","json_list = []\n","\n","# Process each frame to calculate overlaps\n","for frame_id, boxes in frame_data.items():\n","    bboxes = [[box[0], box[1], box[2], box[3]] for box in boxes]\n","    overlap_counts = box_overlap_count(bboxes)\n","\n","    for box in boxes:\n","        json_obj = {\n","            \"video\": video_name,\n","            \"frame\": frame_id,\n","            \"people_id\": box[4],\n","            \"bbox\": [round(box[0], 2), round(box[1], 2), round(box[2], 2), round(box[3], 2)],\n","            \"bbox_area\": round(box[5],2),\n","            \"n_overlapping_boxes\": overlap_counts,\n","            \"score\":  box[6]\n","        }\n","        json_list.append(json_obj)\n","\n","# Convert list to JSON and write to file\n","json_data = json.dumps(json_list, indent=4)\n","with open(output_json , 'w') as json_file:\n","    json_file.write(json_data)"],"metadata":{"id":"E-3DqS74TDTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bMNT0Afylwhl"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}