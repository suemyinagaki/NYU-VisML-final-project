{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19024,"status":"ok","timestamp":1714603041966,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"-TMBd8zBa66X","outputId":"cbc7ce1e-8740-4d4f-a41c-062568e0c0d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/visML_project')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4223,"status":"ok","timestamp":1714603046186,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"kGz5hBkcR9I-","outputId":"7ce84a3c-a04b-4e09-97e0-9458ec01d0da"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","True\n"]}],"source":["import torch\n","print(torch.cuda.is_available())\n","print(torch.backends.cudnn.enabled)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1714603049476,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"NBof3oKlmGaD","outputId":"6e8338b5-2bbb-4774-8505-ecd26375a306"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/visML_project/BoT-SORT\n"]}],"source":["cd BoT-SORT/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234955,"status":"ok","timestamp":1714575139189,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"BdpeNyL6SEbS","outputId":"19e2c82c-d39f-4cd9-95b8-f5a02af5e9f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.25.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.8.0.76)\n","Collecting loguru (from -r requirements.txt (line 3))\n","  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.19.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.66.2)\n","Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.17.1+cu121)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (9.4.0)\n","Collecting thop (from -r requirements.txt (line 9))\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Collecting ninja (from -r requirements.txt (line 10))\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.9.0)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.15.2)\n","Collecting lap (from -r requirements.txt (line 13))\n","  Downloading lap-0.4.0.tar.gz (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting motmetrics (from -r requirements.txt (line 14))\n","  Downloading motmetrics-1.4.0-py3-none-any.whl (161 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting filterpy (from -r requirements.txt (line 15))\n","  Downloading filterpy-1.4.5.zip (177 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (3.9.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (3.7.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (1.11.4)\n","Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.10.0)\n","Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (1.13)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (6.0.1)\n","Collecting yacs (from -r requirements.txt (line 23))\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (2.4.0)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (5.1.0)\n","Collecting onnx==1.8.1 (from -r requirements.txt (line 26))\n","  Downloading onnx-1.8.1.tar.gz (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime==1.8.0 (from versions: 1.12.0, 1.12.1, 1.13.1, 1.14.0, 1.14.1, 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.17.0, 1.17.1, 1.17.3)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime==1.8.0\u001b[0m\u001b[31m\n","\u001b[0mCollecting onnxruntime==1.12.0\n","  Downloading onnxruntime-1.12.0-cp310-cp310-manylinux_2_27_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coloredlogs (from onnxruntime==1.12.0)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0) (24.3.25)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0) (24.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0) (1.12)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.12.0)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.12.0) (1.3.0)\n","Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.12.0\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.10)\n","Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n","  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-o5w_x3ya\n","  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-o5w_x3ya\n","  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (67.7.2)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (3.0.10)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0) (1.16.0)\n","Building wheels for collected packages: pycocotools\n","  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycocotools: filename=pycocotools-2.0-cp310-cp310-linux_x86_64.whl size=375614 sha256=b1439077796eb685d6820e61cfde19df47857425b980286442efb34997561d58\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9s2_u03h/wheels/39/61/b4/480fbddb4d3d6bc34083e7397bc6f5d1381f79acc68e9f3511\n","Successfully built pycocotools\n","Installing collected packages: pycocotools\n","  Attempting uninstall: pycocotools\n","    Found existing installation: pycocotools 2.0.7\n","    Uninstalling pycocotools-2.0.7:\n","      Successfully uninstalled pycocotools-2.0.7\n","Successfully installed pycocotools-2.0\n","Collecting cython_bbox\n","  Downloading cython_bbox-0.1.5.tar.gz (4.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from cython_bbox) (3.0.10)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cython_bbox) (1.25.2)\n","Building wheels for collected packages: cython_bbox\n","  Building wheel for cython_bbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cython_bbox: filename=cython_bbox-0.1.5-cp310-cp310-linux_x86_64.whl size=99134 sha256=902fba85ff7b4307e6249982ebaef4f49e67d6e33727e001cb9c4eeb5df1eb9a\n","  Stored in directory: /root/.cache/pip/wheels/c0/b7/68/bab98b7180cda501101a57fb7d36884218ad45ec60c27cd679\n","Successfully built cython_bbox\n","Installing collected packages: cython_bbox\n","Successfully installed cython_bbox-0.1.5\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n","Installing collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.8.0\n","Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.2\n","running develop\n","/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n","!!\n","\n","        ********************************************************************************\n","        Please avoid running ``setup.py`` and ``easy_install``.\n","        Instead, use pypa/build, pypa/installer, pypa/build or\n","        other standards-based tools.\n","\n","        See https://github.com/pypa/setuptools/issues/917 for details.\n","        ********************************************************************************\n","\n","!!\n","  easy_install.initialize_options(self)\n","/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n","!!\n","\n","        ********************************************************************************\n","        Please avoid running ``setup.py`` directly.\n","        Instead, use pypa/build, pypa/installer, pypa/build or\n","        other standards-based tools.\n","\n","        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n","        ********************************************************************************\n","\n","!!\n","  self.initialize_options()\n","running egg_info\n","writing yolox.egg-info/PKG-INFO\n","writing dependency_links to yolox.egg-info/dependency_links.txt\n","writing top-level names to yolox.egg-info/top_level.txt\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","  warnings.warn(msg.format('we could not find ninja.'))\n","reading manifest file 'yolox.egg-info/SOURCES.txt'\n","adding license file 'LICENSE'\n","writing manifest file 'yolox.egg-info/SOURCES.txt'\n","running build_ext\n","Creating /usr/local/lib/python3.10/dist-packages/yolox.egg-link (link to .)\n","Adding yolox 0.1.0 to easy-install.pth file\n","\n","Installed /content/drive/MyDrive/visML_project/BoT-SORT\n","Processing dependencies for yolox==0.1.0\n","Finished processing dependencies for yolox==0.1.0\n","Collecting loguru\n","  Using cached loguru-0.7.2-py3-none-any.whl (62 kB)\n","Installing collected packages: loguru\n","Successfully installed loguru-0.7.2\n","Collecting thop\n","  Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->thop)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->thop)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->thop)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->thop)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->thop)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->thop)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->thop)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->thop)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->thop)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch->thop)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->thop)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->thop)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 thop-0.1.1.post2209072238\n","Collecting lap\n","  Using cached lap-0.4.0.tar.gz (1.5 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: lap\n","  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lap: filename=lap-0.4.0-cp310-cp310-linux_x86_64.whl size=1628946 sha256=323e7dda6987911551e2083547080b18fe470a5619aac27139993427954f6edb\n","  Stored in directory: /root/.cache/pip/wheels/00/42/2e/9dfe19270eea279d79e84767ff0d7b8082c3bf776cad00e83d\n","Successfully built lap\n","Installing collected packages: lap\n","Successfully installed lap-0.4.0\n","Collecting yacs\n","  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs) (6.0.1)\n","Installing collected packages: yacs\n","Successfully installed yacs-0.1.8\n"]}],"source":["!pip3 install -r requirements.txt\n","!pip install onnxruntime==1.12.0\n","!pip3 install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","!pip3 install cython_bbox\n","!pip3 install faiss-cpu\n","!pip3 install faiss-gpu\n","!python3 setup.py develop\n","!pip install loguru\n","!pip install thop\n","!pip install lap\n","!pip install yacs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1714083675611,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"iQjruGM763KW","outputId":"ee4def68-82da-4738-af10-9e54e604959c"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/visML_project/BoT-SORT\n"]}],"source":["cd BoT-SORT/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":215,"status":"ok","timestamp":1714083678276,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"XHtYkVTUSoPQ","outputId":"c51142d1-f52c-45d9-fd7a-23a71fce6be1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34mBoT-SORT\u001b[0m/                 \u001b[01;34moutput_frames\u001b[0m/  \u001b[01;34mstreetaware_sample_chase_1_sensor_1_left\u001b[0m/\n","bot_sort.ipynb            \u001b[01;34moutputs\u001b[0m/        \u001b[01;34mstreetaware_sample_chase_1_sensor_1_right\u001b[0m/\n","data_preprocessing.ipynb  \u001b[01;34moutput_videos\u001b[0m/  \u001b[01;34mstreetaware_sample_chase_1_sensor_2_left\u001b[0m/\n","\u001b[01;34mMOT17\u001b[0m/                    \u001b[01;34mStreetAware\u001b[0m/    \u001b[01;34mstreetaware_sample_chase_1_sensor_2_right\u001b[0m/\n"]}],"source":["ls '/content/drive/My Drive/visML_project/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":399180,"status":"ok","timestamp":1714575830433,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"4HW3zWM1wIsS","outputId":"06a50208-0e43-4c3e-f9e6-71dbceb369db"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-05-01 14:58:17.410305: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-01 14:58:17.410384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-01 14:58:17.552343: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-01 14:58:19.691615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2024-05-01 14:58:59.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m228\u001b[0m - \u001b[1mArgs: Namespace(path='/content/drive/My Drive/visML_project/streetaware_sample_chase_1_sensor_1_right/test/MOT17-01-FRCNN/img1', benchmark='MOT17', split_to_eval='test', exp_file='./yolox/exps/example/mot/yolox_x_mix_det.py', ckpt='./pretrained/bytetrack_x_mot17.pth.tar', experiment_name='yolox_x_mix_det', default_parameters=True, save_frames=False, device=device(type='cuda'), conf=None, nms=None, tsize=None, fp16=True, fuse=True, track_high_thresh=0.65, track_low_thresh=0.1, new_track_thresh=0.75, track_buffer=30, match_thresh=0.8, aspect_ratio_thresh=1.6, min_box_area=10, cmc_method='file', with_reid=False, fast_reid_config='fast_reid/configs/MOT17/sbs_S50.yml', fast_reid_weights='pretrained/mot17_sbs_S50.pth', proximity_thresh=0.5, appearance_thresh=0.25, name='MOT17-01-FRCNN', ablation=False, mot20=False, fps=30, batch_size=1, trt=False)\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\u001b[32m2024-05-01 14:59:09.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mModel Summary: Params: 99.00M, Gflops: 793.21\u001b[0m\n","\u001b[32m2024-05-01 14:59:09.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1mloading checkpoint\u001b[0m\n","\u001b[32m2024-05-01 15:00:05.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mloaded checkpoint done.\u001b[0m\n","\u001b[32m2024-05-01 15:00:05.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m253\u001b[0m - \u001b[1m\tFusing model...\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:836: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n","  if param.grad is not None:\n","\u001b[32m2024-05-01 15:00:42.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 20/900 (10.99 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:00:46.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 40/900 (15.31 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:00:52.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 60/900 (17.45 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:00:55.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 80/900 (18.94 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:00:59.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 100/900 (19.95 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:04.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 120/900 (20.60 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:09.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 140/900 (20.79 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:13.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 160/900 (21.21 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:18.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 180/900 (21.59 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:22.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 200/900 (21.83 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:26.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 220/900 (22.13 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:30.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 240/900 (22.38 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:35.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 260/900 (22.52 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:39.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 280/900 (22.71 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:43.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 300/900 (22.88 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:47.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 320/900 (23.01 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:51.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 340/900 (23.12 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:55.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 360/900 (23.23 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:01:59.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 380/900 (23.33 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:04.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 400/900 (23.24 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:08.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 420/900 (23.32 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:12.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 440/900 (23.41 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:16.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 460/900 (23.46 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:20.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 480/900 (23.53 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:24.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 500/900 (23.59 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:29.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 520/900 (23.63 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:32.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 540/900 (23.68 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:36.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 560/900 (23.74 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:41.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 580/900 (23.77 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:45.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 600/900 (23.81 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:49.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 620/900 (23.85 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:53.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 640/900 (23.88 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:02:57.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 660/900 (23.91 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:01.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 680/900 (23.95 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:05.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 700/900 (23.97 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:10.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 720/900 (23.99 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:14.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 740/900 (24.02 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:17.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 760/900 (24.04 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:22.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 780/900 (24.04 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:26.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 800/900 (24.07 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:30.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 820/900 (24.10 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:35.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 840/900 (24.11 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:39.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 860/900 (24.15 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:42.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 880/900 (24.18 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:47.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1mProcessing frame 900/900 (24.20 fps)\u001b[0m\n","\u001b[32m2024-05-01 15:03:47.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_track\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1msave results to ./YOLOX_outputs/yolox_x_mix_det/track_results/MOT17-01-FRCNN.txt\u001b[0m\n","TOTAL TIME END-to-END (with loading networks and images):  288.2079668045044\n","TOTAL TIME (Detector + Tracker): 37.18468904495239, FPS: 24.203510184312538\n","TOTAL TIME (Tracker only): 1.8194398880004883, FPS: 494.65772732347534\n"]}],"source":["!python3 tools/track.py \"/content/drive/My Drive/visML_project/streetaware_sample_chase_1_sensor_1_right\" --default-parameters --fast-reid-weights pretrained/mot17_sbs_S50.pth --benchmark \"MOT17\" --eval \"test\" --fp16 --fuse\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1714577039915,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"jzenFmaruqDp","outputId":"ac19c797-07ad-42dd-af72-8eba4678443b"},"outputs":[{"name":"stdout","output_type":"stream","text":["left_people_track.txt  right_people_track.json  right_people_track.txt\n"]}],"source":["ls 'outputs/botsort/chase_1/sensor_1/right_people_track.txt'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvT7QPz9DG7V"},"outputs":[],"source":[" write_results_score(save_seq_txt, seq_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1713340736669,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"WmK41r8nD7ST","outputId":"da5fd2f1-1b94-4540-cc46-edd04c8c21e3"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'MOT17-01-FRCNN.txt'"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["save_seq_txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1714577449985,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"y9gEg4qjD2Ko","outputId":"ee642b92-712a-451b-d7e6-503767ea5510"},"outputs":[{"name":"stdout","output_type":"stream","text":["left_people_track.txt  right_people_track.json  right_people_track.txt\n"]}],"source":["ls  '/content/drive/My Drive/visML_project/outputs/botsort/chase_1/sensor_1/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":5121,"status":"error","timestamp":1714577660240,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"QnuxFYDP8nt1","outputId":"f1dbe081-dca1-49d5-bfca-093646e35fd6"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","\n","# Load tracking results\n","track_file = '/content/drive/My Drive/visML_project/outputs/botsort/chase_1/sensor_1/right_people_track.txt'\n","tracks = {}\n","with open(track_file, 'r') as file:\n","    for line in file:\n","        frame_id, track_id, x, y, w, h, conf, _, _, _ = map(float, line.strip().split(','))\n","        if frame_id not in tracks:\n","            tracks[frame_id] = []\n","        tracks[frame_id].append((track_id, x, y, w, h, conf))\n","\n","# Assign a unique color to each track_id\n","color_map = {}\n","for frame in tracks.values():\n","    for track_id, *_ in frame:\n","        if track_id not in color_map:\n","            color_map[track_id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n","\n","# Directory containing frames\n","frames_dir = '/content/drive/My Drive/visML_project/streetaware_sample_chase_1_sensor_1_right/test/MOT17-01-FRCNN/img1/'\n","\n","# Output directory\n","output_dir = '/content/drive/My Drive/visML_project/output_frames/sensor_2_left'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Scale factor for resizing\n","scale_factor = 0.5\n","\n","# Process each frame\n","for frame_file in sorted(os.listdir(frames_dir)):\n","    frame_path = os.path.join(frames_dir, frame_file)\n","    frame = cv2.imread(frame_path)\n","    if frame is None:\n","        continue\n","\n","    # Resize frame\n","    frame = cv2.resize(frame, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)\n","\n","    frame_id = int(frame_file.split('.')[0])  # assuming frame filenames are like '000001.jpg'\n","    if frame_id in tracks:\n","        for track_id, x, y, w, h, conf in tracks[frame_id]:\n","            # Scale bounding box coordinates and size\n","            x *= scale_factor\n","            y *= scale_factor\n","            w *= scale_factor\n","            h *= scale_factor\n","            color = color_map[track_id]\n","            cv2.rectangle(frame, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n","            cv2.putText(frame, f'ID: {int(track_id)}, Conf: {conf:.2f}', (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","\n","    # Save the frame\n","    output_path = os.path.join(output_dir, frame_file)\n","    cv2.imwrite(output_path, frame)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EnJfzAMfRfq"},"outputs":[],"source":["from google.colab.patches import cv2_imshow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_sytc1DcIpxR"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","\n","# Load tracking results\n","track_file = '/content/drive/My Drive/visML_project/outputs/right_people_track.txt'\n","tracks = {}\n","with open(track_file, 'r') as file:\n","    for line in file:\n","        frame_id, track_id, x, y, w, h, conf, _, _, _ = map(float, line.strip().split(','))\n","        if frame_id not in tracks:\n","            tracks[frame_id] = []\n","        tracks[frame_id].append((track_id, x, y, w, h, conf))\n","\n","# Assign a unique color to each track_id\n","color_map = {}\n","for frame in tracks.values():\n","    for track_id, *_ in frame:\n","        if track_id not in color_map:\n","            color_map[track_id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n","\n","# Directory containing frames\n","frames_dir = '/content/drive/My Drive/visML_project/streetaware_sample_chase_1_sensor_2_right/test/MOT17-01-FRCNN/img1/'\n","\n","# Output directory\n","output_dir = '/content/drive/My Drive/visML_project/output_frames/sensor_2_right'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Scale factor for resizing\n","scale_factor = 1\n","\n","# Process each frame\n","for frame_file in sorted(os.listdir(frames_dir)):\n","    frame_path = os.path.join(frames_dir, frame_file)\n","    frame = cv2.imread(frame_path)\n","    if frame is None:\n","        continue\n","\n","    # Resize frame\n","    frame = cv2.resize(frame, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)\n","\n","    frame_id = int(frame_file.split('.')[0])  # assuming frame filenames are like '000001.jpg'\n","    if frame_id in tracks:\n","        for track_id, x, y, w, h, conf in tracks[frame_id]:\n","            # Scale bounding box coordinates and size\n","            x *= scale_factor\n","            y *= scale_factor\n","            w *= scale_factor\n","            h *= scale_factor\n","            color = color_map[track_id]\n","            cv2.rectangle(frame, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n","            cv2.putText(frame, f'ID: {int(track_id)}, Conf: {conf:.2f}', (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","\n","    # Save the frame\n","    output_path = os.path.join(output_dir, frame_file)\n","    cv2.imwrite(output_path, frame)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":763,"status":"ok","timestamp":1714001302592,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"bOth6C23JkWX","outputId":"89fec5de-f4f1-4035-af01-0d5f20fc0b0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["array.json  left_half.mp4  left_people_cleaned.json     left_quarter.mp4  right.mp4\n","\u001b[0m\u001b[01;34mimg_left\u001b[0m/   left.json      left_people_cleaned_mot.txt  right_half.mp4    right_people.json\n","\u001b[01;34mimg_right\u001b[0m/  left.mp4       left_people.json             right.json        right_quarter.mp4\n"]}],"source":["ls '/content/drive/My Drive/visML_project/StreetAware/sensor_1/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"93I6qyJ7AHWe"},"outputs":[],"source":["json_file_path = 'StreetAware/sensor_1/left_people.json'\n","with open(json_file_path, 'r') as file:\n","    data = json.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163,"status":"ok","timestamp":1714083318589,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"OWjFKn4LIP1M","outputId":"fc9e2906-8486-49ea-a722-83897f69822f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34mtest\u001b[0m/\n"]}],"source":["ls '/content/drive/My Drive/visML_project/streetaware_sample_chase_1_sensor_2_right/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1jRoVS_0T5FRArZNDEZuiZftDSp60pF6W"},"id":"brqQCajUBzx-","outputId":"2a8a5062-0bbf-4ced-9bb5-7e39e1ec556a"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["import cv2\n","import os\n","from google.colab.patches import cv2_imshow\n","\n","# Path to the directory containing frames\n","frames_dir = '/content/drive/My Drive/visML_project/streetaware_sample_chase_2_sensor_1_right/test/MOT17-01-FRCNN/img1/'\n","\n","# Path to the detections file\n","detections_file ='/content/drive/My Drive/visML_project/streetaware_sample_chase_2_sensor_1_right/test/MOT17-01-FRCNN/det1/det.txt'\n","\n","# Read detections from file\n","detections = {}\n","with open(detections_file, 'r') as f:\n","    for line in f:\n","        frame_id, _, x, y, w, h, confidence = line.strip().split(',')\n","        frame_id, x, y, w, h, confidence = int(frame_id), float(x), float(y), float(w), float(h), float(confidence)\n","        if frame_id not in detections:\n","            detections[frame_id] = []\n","        # Scale the bounding box coordinates and dimensions\n","        scale_factor = 0.5  # Scaling down to 50%\n","        scaled_x = int(x * scale_factor)\n","        scaled_y = int(y * scale_factor)\n","        scaled_w = int(w * scale_factor)\n","        scaled_h = int(h * scale_factor)\n","        detections[frame_id].append((scaled_x, scaled_y, scaled_w, scaled_h, confidence))\n","\n","# Initialize frame counter\n","frame_counter = 0\n","max_frames = 10  # Maximum number of frames to process\n","\n","# Process each frame and draw bounding boxes\n","for frame_id in sorted(detections.keys()):\n","    if frame_counter >= max_frames:\n","        break  # Stop processing after 5 frames\n","\n","    frame_file = f'{frame_id:06d}.jpg'\n","    frame_path = os.path.join(frames_dir, frame_file)\n","    frame = cv2.imread(frame_path)\n","\n","    if frame is None:\n","        print(f\"Frame {frame_file} not found.\")\n","        continue\n","\n","    # Resize the frame\n","    frame = cv2.resize(frame, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n","\n","    # Draw bounding boxes on the frame\n","    for x, y, w, h, confidence in detections[frame_id]:\n","        top_left = (x, y)\n","        bottom_right = (x + w, y + h)\n","        color = (0, 255, 0) if confidence >= 0 else (0, 0, 255)  # Green if confidence >= 0, red otherwise\n","        cv2.rectangle(frame, top_left, bottom_right, color, 2)\n","        cv2.putText(frame, f'Conf: {confidence:.2f}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n","\n","    # display the frame\n","    cv2_imshow(frame)  # Display the frame in Colab\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","    frame_counter += 1  # Increment the frame counter\n","\n","cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714027079722,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"Jq5ypOJ_-Z2m","outputId":"4cc62cb8-41e1-4bcf-b96c-06a1582e6f12"},"outputs":[{"name":"stdout","output_type":"stream","text":[" \u001b[0m\u001b[01;34mBoT-SORT\u001b[0m/                  \u001b[01;34mMOT17\u001b[0m/                \u001b[01;34mStreetAware\u001b[0m/\n"," bot_sort.ipynb            \u001b[01;34m'output_frames (1)'\u001b[0m/   \u001b[01;34mstreetaware_sample_chase_1_sensor_1_left\u001b[0m/\n"," data_preprocessing.ipynb   \u001b[01;34moutputs\u001b[0m/              \u001b[01;34mstreetaware_sample_chase_1_sensor_1_right\u001b[0m/\n"]}],"source":["cd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":5,"status":"error","timestamp":1714086061269,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"TvrO020BEnlB","outputId":"d1e42a11-d7e6-4082-beb4-254f261c862e"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/My Drive/visML_project/StreetAware/sensor_1/left_people.json'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-adffd3f2aa87>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the JSON data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#with open('/content/drive/My Drive/visML_project/StreetAware/sensor_1/right_people.json', 'r') as file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/visML_project/StreetAware/sensor_1/left_people.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/visML_project/StreetAware/sensor_1/left_people.json'"]}],"source":["import cv2\n","import os\n","import json\n","from google.colab.patches import cv2_imshow\n","\n","# Load the JSON data\n","#with open('/content/drive/My Drive/visML_project/StreetAware/sensor_1/right_people.json', 'r') as file:\n","with open('/content/drive/My Drive/visML_project/StreetAware/sensor_1/left_people.json', 'r') as file:\n","    data = json.load(file)\n","\n","# Directory containing frames\n","#frames_dir ='/content/drive/My Drive/visML_project/StreetAware/sensor_1/img_right/'\n","frames_dir ='/content/drive/My Drive/visML_project/StreetAware/sensor_1/img_right/'\n","\n","\n","# Initialize counter for non-null frames\n","non_null_counter = 0\n","max_non_null_frames = 10\n","\n","# Scaling factor\n","scale_factor = 0.5  # Reduce size to 50%\n","\n","# Process each item in the JSON data\n","for frame_index, frame_data in enumerate(data):\n","    if frame_data is None:\n","        continue  # Skip null frames\n","\n","    # Increment the counter and break if maximum frames are processed\n","    non_null_counter += 1\n","    if non_null_counter > max_non_null_frames:\n","        break\n","\n","    frame_file = f'{frame_index + 1:06d}.jpg'  # Frame file names are 1-based index\n","    frame_path = os.path.join(frames_dir, frame_file)\n","    frame = cv2.imread(frame_path)\n","\n","    if frame is None:\n","        print(f\"Frame {frame_file} not found.\")\n","        continue\n","\n","    # Resize the frame\n","    frame = cv2.resize(frame, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n","\n","    # Draw bounding boxes\n","    for box in frame_data['boxes']:\n","        top_left = (int(box[0][0] * scale_factor), int(box[0][1] * scale_factor))\n","        bottom_right = (int(box[1][0] * scale_factor), int(box[1][1] * scale_factor))\n","        cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)  # Green color\n","\n","    # display the frame\n","    cv2_imshow(frame)  # Display the frame in Colab\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1iBpfiJycMq39fNWAWdR2KxDH-N2L7iLs"},"executionInfo":{"elapsed":13353,"status":"ok","timestamp":1714015511679,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"sNxHbDWGGd4L","outputId":"c1b627b8-ec5f-45cf-c252-b2cd20d89976"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["import cv2\n","import os\n","import json\n","from google.colab.patches import cv2_imshow\n","\n","# Load the JSON data\n","with open('/content/drive/My Drive/visML_project/StreetAware/sensor_1/left_people.json', 'r') as file:\n","    data = json.load(file)\n","\n","# Directory containing frames\n","frames_dir = '/content/drive/My Drive/visML_project/StreetAware/sensor_1/img_left/'\n","\n","# Output directory\n","output_dir = '/content/drive/My Drive/visML_project/output_frames'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Initialize counter for non-null frames\n","non_null_counter = 0\n","max_non_null_frames = 10\n","\n","# Scaling factor\n","scale_factor = 0.5  # Reduce size to 50%\n","\n","# Process each item in the JSON data\n","for frame_index, frame_data in enumerate(data):\n","    if frame_data is None:\n","        continue  # Skip null frames\n","\n","    # Increment the counter and break if maximum frames are processed\n","    non_null_counter += 1\n","    if non_null_counter > max_non_null_frames:\n","        break\n","\n","    frame_file = f'{frame_index + 1:06d}.jpg'  # Frame file names are 1-based index\n","    frame_path = os.path.join(frames_dir, frame_file)\n","    frame = cv2.imread(frame_path)\n","\n","    if frame is None:\n","        print(f\"Frame {frame_file} not found.\")\n","        continue\n","\n","    # Resize the frame\n","    frame = cv2.resize(frame, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n","\n","    # Draw bounding boxes and display confidence\n","    scores = frame_data.get('scores', [])  # Retrieve scores if they exist\n","    for box_index, box in enumerate(frame_data['boxes']):\n","        top_left = (int(box[0][0] * scale_factor), int(box[0][1] * scale_factor))\n","        bottom_right = (int(box[1][0] * scale_factor), int(box[1][1] * scale_factor))\n","\n","        # Determine the color based on the availability of a score\n","        if box_index < len(scores):\n","            color = (0, 255, 0)  # Green for boxes with scores\n","            score = scores[box_index]\n","            text = f'{score:.2f}'\n","        else:\n","            color = (0, 0, 255)  # Red for boxes without scores\n","            text = 'N/A'\n","\n","        cv2.rectangle(frame, top_left, bottom_right, color, 2)\n","        cv2.putText(frame, text, (top_left[0], top_left[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n","\n","    # Save or display the frame\n","    output_path = os.path.join(output_dir, frame_file)\n","    cv2.imwrite(output_path, frame)\n","    cv2_imshow(frame)  # Display the frame in Colab\n","    if cv2.waitKey(25) & 0xFF == ord('q'):\n","        break\n","\n","cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":511,"status":"ok","timestamp":1714086760377,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"LZBwNSdKBCgn","outputId":"6fd04026-795d-4f51-9283-f35142102fb0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34msensor_1_left\u001b[0m/  \u001b[01;34msensor_1_right\u001b[0m/  \u001b[01;34msensor_2_left\u001b[0m/  \u001b[01;34msensor_2_right\u001b[0m/\n"]}],"source":["ls output_frames/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140556,"status":"ok","timestamp":1714086943441,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"Y69bzD26-X55","outputId":"8a2193c8-677c-46e4-d4b4-ba47cee13875"},"outputs":[{"name":"stdout","output_type":"stream","text":["Video saved to output_videos/sensor_2_left.mp4\n"]}],"source":["import cv2\n","import os\n","\n","# Directory containing frames\n","frames_dir = 'output_frames/sensor_2_left/'\n","\n","# Output directory\n","output_dir = 'output_videos/'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Video setup\n","output_video_path = os.path.join(output_dir, 'sensor_2_left.mp4')\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 files\n","fps = 30\n","\n","# Assume first frame to get video dimensions\n","first_frame_path = os.path.join(frames_dir, '000001.jpg')\n","first_frame = cv2.imread(first_frame_path)\n","frame_height, frame_width = first_frame.shape[:2]\n","video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n","\n","# Process each frame\n","sorted_frame_files = sorted(os.listdir(frames_dir))  # Sort to ensure correct frame order\n","for frame_file in sorted_frame_files:\n","    if frame_file.endswith('.jpg'):  # Check for jpeg images\n","        frame_path = os.path.join(frames_dir, frame_file)\n","        frame = cv2.imread(frame_path)\n","        if frame is None:\n","            continue\n","        video_writer.write(frame)\n","\n","# Release the VideoWriter\n","video_writer.release()\n","print(f\"Video saved to {output_video_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124224,"status":"ok","timestamp":1714087067659,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"0dvexnWhBhsP","outputId":"e093846d-88a3-4716-a16d-e849b39f0165"},"outputs":[{"name":"stdout","output_type":"stream","text":["Video saved to output_videos/sensor_2_right.mp4\n"]}],"source":["import cv2\n","import os\n","\n","# Directory containing frames\n","frames_dir = 'output_frames/sensor_2_right/'\n","\n","# Output directory\n","output_dir = 'output_videos/'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Video setup\n","output_video_path = os.path.join(output_dir, 'sensor_2_right.mp4')\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 files\n","fps = 30\n","\n","# Assume first frame to get video dimensions\n","first_frame_path = os.path.join(frames_dir, '000001.jpg')\n","first_frame = cv2.imread(first_frame_path)\n","frame_height, frame_width = first_frame.shape[:2]\n","video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n","\n","# Process each frame\n","sorted_frame_files = sorted(os.listdir(frames_dir))  # Sort to ensure correct frame order\n","for frame_file in sorted_frame_files:\n","    if frame_file.endswith('.jpg'):  # Check for jpeg images\n","        frame_path = os.path.join(frames_dir, frame_file)\n","        frame = cv2.imread(frame_path)\n","        if frame is None:\n","            continue\n","        video_writer.write(frame)\n","\n","# Release the VideoWriter\n","video_writer.release()\n","print(f\"Video saved to {output_video_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":608,"status":"ok","timestamp":1714568972226,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"nMMlzFl8j5mQ","outputId":"68501010-ea11-4860-aeaf-f5b3b5ba755d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34mBoT-SORT\u001b[0m/                 \u001b[01;34mstreetaware_sample_chase_1_sensor_1_left\u001b[0m/\n","bot_sort.ipynb            \u001b[01;34mstreetaware_sample_chase_1_sensor_1_right\u001b[0m/\n","\u001b[01;34mbytetrack\u001b[0m/                \u001b[01;34mstreetaware_sample_chase_1_sensor_2_left\u001b[0m/\n","\u001b[01;34mbytetrack_output_frames\u001b[0m/  \u001b[01;34mstreetaware_sample_chase_1_sensor_2_right\u001b[0m/\n","data_preprocessing.ipynb  \u001b[01;34mstreetaware_sample_chase_2_sensor_1_left\u001b[0m/\n","\u001b[01;34minput\u001b[0m/                    \u001b[01;34mstreetaware_sample_chase_2_sensor_1_right\u001b[0m/\n","\u001b[01;34mMOT17\u001b[0m/                    \u001b[01;34mstreetaware_sample_chase_2_sensor_2_left\u001b[0m/\n","\u001b[01;34moutput_frames\u001b[0m/            \u001b[01;34mstreetaware_sample_chase_2_sensor_2_right\u001b[0m/\n","\u001b[01;34moutputs\u001b[0m/                  yolov8l.pt\n","\u001b[01;34moutput_videos\u001b[0m/            yolov8n.pt\n","\u001b[01;34mStreetAware\u001b[0m/              yolov8x.pt\n"]}],"source":["ls"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1714603294549,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"oYq_zwDGo-05"},"outputs":[],"source":["def box_overlap_count(boxes):\n","    def overlap(box1, box2):\n","        # Check if box1 overlaps with box2\n","        x1_min, y1_min, x1_max, y1_max = box1\n","        x2_min, y2_min, x2_max, y2_max = box2\n","        return not (x1_max < x2_min or x2_max < x1_min or y1_max < y2_min or y2_max < y1_min)\n","\n","    count = 0\n","    for i in range(len(boxes)):\n","        for j in range(i + 1, len(boxes)):\n","            if overlap(boxes[i], boxes[j]):\n","                count += 1\n","    return count\n"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":228,"status":"ok","timestamp":1714603727638,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"nFKHVIBWiT4T"},"outputs":[],"source":["import json\n","\n","video_name = \"streetaware_sample_chase_2_sensor_2_right\"\n","txt_path = 'outputs/botsort/chase_2/sensor_2/right_people_track.txt'\n","output_json = 'outputs/botsort/chase_2/sensor_2/right_people_track.json'\n","frame_data = {}\n","\n","# Reading data from file\n","with open(txt_path, 'r') as file:\n","    data = file.read()\n","\n","for line in data.strip().split(\"\\n\"):\n","    parts = line.split(',')\n","    frame_id = int(parts[0])\n","    people_id = int(parts[1])\n","    x1 = float(parts[2])\n","    y1 = float(parts[3])\n","    width = float(parts[4])\n","    height = float(parts[5])\n","    conf = float(parts[6])\n","    x2 = x1 + width\n","    y2 = y1 + height\n","    bbox_area = width * height\n","    if frame_id not in frame_data:\n","        frame_data[frame_id] = []\n","    frame_data[frame_id].append([x1, y1, x2, y2, people_id, bbox_area, conf])"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1714603729531,"user":{"displayName":"Yichen Shi","userId":"06657077970695268431"},"user_tz":240},"id":"E-3DqS74TDTj"},"outputs":[],"source":["\n","json_list = []\n","\n","# Process each frame to calculate overlaps\n","for frame_id, boxes in frame_data.items():\n","    bboxes = [[box[0], box[1], box[2], box[3]] for box in boxes]\n","    overlap_counts = box_overlap_count(bboxes)\n","\n","    for box in boxes:\n","        json_obj = {\n","            \"video\": video_name,\n","            \"frame\": frame_id,\n","            \"people_id\": box[4],\n","            \"bbox\": [round(box[0], 2), round(box[1], 2), round(box[2], 2), round(box[3], 2)],\n","            \"bbox_area\": round(box[5],2),\n","            \"n_overlapping_boxes\": overlap_counts,\n","            \"score\":  box[6]\n","        }\n","        json_list.append(json_obj)\n","\n","# Convert list to JSON and write to file\n","json_data = json.dumps(json_list, indent=4)\n","with open(output_json , 'w') as json_file:\n","    json_file.write(json_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMNT0Afylwhl"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
